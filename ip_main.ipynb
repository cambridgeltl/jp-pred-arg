{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bp_pas.params as params\n",
    "import bp_pas.corpus as corpus\n",
    "import bp_pas.stats as stats\n",
    "from bp_pas.eval import evaluate\n",
    "from bp_pas.util.vocab import Vocab\n",
    "\n",
    "import sys\n",
    "import gflags\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "params.load_defaults()\n",
    "flags = gflags.FLAGS\n",
    "#arg_str = '' #'--train_data single.utf8 --dev_data single.utf8 --test_data single.utf8'\n",
    "arg_str = '--max_train_instances 1 --use_context=true --use_sp=true --sp_dims 20 --context_dims 50 --num_context_layers 8' #'--train_data single.utf8 --dev_data single.utf8 --test_data single.utf8'\n",
    "args = [''] + arg_str.split(' ')\n",
    "argv = flags(args)\n",
    "print(flags.use_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp params to add to flags\n",
    "unk_threshold = 1\n",
    "unk_token = '<UNK>'\n",
    "mask_token = '<PRED>'\n",
    "\n",
    "#if not flags.use_context:\n",
    "#    flags.context_dims = 0\n",
    "#if not flags.use_sp:\n",
    "#    flags.sp_dims = 0\n",
    "\n",
    "synt_arg_embedding_size = flags.context_dims\n",
    "sem_arg_embedding_size = flags.sp_dims\n",
    "\n",
    "#complete_embedding_size = synt_arg_embedding_size + sem_arg_embedding_size\n",
    "num_training_instances = 1\n",
    "\n",
    "assert synt_arg_embedding_size % 2 == 0\n",
    "#print(complete_embedding_size)\n",
    "\n",
    "print(synt_arg_embedding_size)\n",
    "print(sem_arg_embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/NTC_1.5/processed/train.utf8\n",
      "1 train sentences.\n",
      "1 test sentences.\n",
      "1 dev sentences.\n",
      "\n",
      "CORPUS STATISTICS\n",
      "\tDocs: 1  Sents: 1  Words: 71\n",
      "\tPredicates: 9  Arguments 15\n",
      "\n",
      "\n",
      "\n",
      "CASE DISTRIBUTION\n",
      "\tGa\tBST: 0  DEP: 2  INTRA-ZERO: 7  INTER-ZERO: 0  EXOPHORA: 0\n",
      "\tO\tBST: 0  DEP: 2  INTRA-ZERO: 0  INTER-ZERO: 0  EXOPHORA: 0\n",
      "\tNi\tBST: 0  DEP: 4  INTRA-ZERO: 0  INTER-ZERO: 0  EXOPHORA: 0\n",
      "\n",
      "\tPredicates: 9\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(flags.train_data)\n",
    "ntc = corpus.NTCLoader()\n",
    "train_data = ntc.load_corpus(flags.train_data, flags.max_train_instances)\n",
    "test_data  = ntc.load_corpus(flags.test_data, flags.max_train_instances)\n",
    "dev_data   = ntc.load_corpus(flags.dev_data, flags.max_train_instances)\n",
    "print('{} train sentences.'.format(len(train_data)))\n",
    "print('{} test sentences.'.format(len(test_data)))\n",
    "print('{} dev sentences.'.format(len(dev_data)))\n",
    "stats.corpus_statistics(train_data)\n",
    "stats.show_case_dist(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 train sentences.\n",
      "1 test sentences.\n",
      "1 dev sentences.\n"
     ]
    }
   ],
   "source": [
    "# Unpack data from \"docs\"->sents\n",
    "train_data = train_data[0]\n",
    "test_data  = test_data[0]\n",
    "dev_data   = dev_data[0]\n",
    "print('{} train sentences.'.format(len(train_data)))\n",
    "print('{} test sentences.'.format(len(test_data)))\n",
    "print('{} dev sentences.'.format(len(dev_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total sentences: 3\n",
      "Collecting all tokens...\n",
      "Total tokens: 157\n",
      "\n",
      "Building (argument) vocabulary...\n",
      "Vocabulary size: 81\n",
      "\n",
      "Collecting predicates...\n",
      "Number of predicates: 14\n",
      "\n",
      "Collecting argument types...\n",
      "Arg types:  ['NIL', 'O', 'NI', 'GA']\n"
     ]
    }
   ],
   "source": [
    "# Setup vocabulary\n",
    "all_sents = train_data + test_data + dev_data\n",
    "print('Number of total sentences: {}'.format(len(all_sents)))\n",
    "\n",
    "print('Collecting all tokens...')\n",
    "word_tokens = list(word.form\n",
    "                   for sent in all_sents\n",
    "                   for word in sent)\n",
    "print('Total tokens: {}\\n'.format(len(word_tokens)))\n",
    "\n",
    "print('Building (argument) vocabulary...')\n",
    "arg_vocab = Vocab(init_words=[mask_token] + word_tokens, \n",
    "                  unk_token=unk_token, \n",
    "                  unk_threshold=unk_threshold)\n",
    "arg_vocab.freeze()\n",
    "arg_vocabulary_size = len(arg_vocab)\n",
    "print('Vocabulary size: {}\\n'.format(arg_vocabulary_size))\n",
    "\n",
    "print('Collecting predicates...')\n",
    "pred_vocab = Vocab(init_words=[word.form\n",
    "                               for sent in all_sents\n",
    "                               for word in sent if word.is_prd])\n",
    "pred_vocab.freeze()\n",
    "pred_vocabulary_size = len(pred_vocab)\n",
    "print('Number of predicates: {}\\n'.format(pred_vocabulary_size))\n",
    "\n",
    "print('Collecting argument types...')\n",
    "arg_types = ['NIL'] + list(set([arg.arg_type \n",
    "                      for sent in all_sents \n",
    "                      for pas in sent.pas\n",
    "                      for arg in pas.args]))\n",
    "print('Arg types: ', arg_types)\n",
    "num_types = len(arg_types)\n",
    "\n",
    "#print(pred_vocab.elems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Convert data structures into numpy ndarrays for placeholders\n",
    "from bp_pas.util.feed_dict_helper import separate_pred_dicts\n",
    "\n",
    "train_dicts = [ex \n",
    "               for td in train_data \n",
    "               for ex in separate_pred_dicts(td, pred_vocab, arg_vocab, arg_types, mask_token)]\n",
    "\n",
    "test_dicts = [separate_pred_dicts(td, pred_vocab, arg_vocab, arg_types, mask_token)\n",
    "              for td in train_data]\n",
    "\n",
    "# Just for now\n",
    "test_data = train_data\n",
    "print(len(train_dicts))\n",
    "print(len(test_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the context embedding\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Construct syntactic embedding matrix for contex/arguments\n",
    "synt_arg_embeddings = tf.Variable(tf.random_uniform([arg_vocabulary_size, synt_arg_embedding_size], -1.0, 1.0, dtype=tf.float64), trainable=True)\n",
    "\n",
    "# Construct semantic embedding matrix for selectional preference\n",
    "sem_arg_embeddings = tf.Variable(tf.random_uniform([arg_vocabulary_size, sem_arg_embedding_size], -1.0, 1.0, dtype=tf.float64), trainable=True)\n",
    "\n",
    "# Construct embedding matrix for predicates\n",
    "synt_pred_embeddings = tf.Variable(tf.random_uniform([pred_vocabulary_size, num_types * synt_arg_embedding_size], -1.0, 1.0, dtype=tf.float64), trainable=True)\n",
    "sem_pred_embeddings = tf.Variable(tf.random_uniform([pred_vocabulary_size, num_types * sem_arg_embedding_size], -1.0, 1.0, dtype=tf.float64), trainable=True)\n",
    "\n",
    "# Setup placeholders\n",
    "batch_size = 1\n",
    "sent_placeholder = tf.placeholder(tf.int32, shape=[None])\n",
    "pred_placeholder = tf.placeholder(tf.int32, shape=[None])\n",
    "gold_placeholder = tf.placeholder(tf.float64, shape=[num_types, None])\n",
    "\n",
    "from bp_pas.model.baseline import Model\n",
    "\n",
    "model = Model(synt_pred_embeddings, synt_arg_embeddings, \n",
    "              sem_pred_embeddings, sem_arg_embeddings,\n",
    "              sent_placeholder, pred_placeholder, gold_placeholder, flags)\n",
    "\n",
    "\n",
    "scored_mat_op = model.new_pred_full_scoring(sent_placeholder, pred_placeholder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def meta_decode(sess, instance):\n",
    "#    print('In decode')\n",
    "    # Collect model-scores labels-x-tokens mats\n",
    "    int_inputs = separate_pred_dicts(instance, pred_vocab, arg_vocab, arg_types, mask_token)\n",
    "#    print('plen: ', len(int_inputs))\n",
    "    scored_mats = []\n",
    "    for sent_ids, pred_ids, gold_labels in int_inputs:\n",
    "        feed_dict={\n",
    "            sent_placeholder: sent_ids,\n",
    "            pred_placeholder: pred_ids,\n",
    "            gold_placeholder: gold_labels\n",
    "        }\n",
    "        scored_mat = sess.run(scored_mat_op.output, feed_dict=feed_dict)\n",
    "        scored_mats.append(scored_mat)\n",
    "#        print(gold_labels)\n",
    "#        print('\\n')\n",
    "#        print(scored_mats[-1])\n",
    "#        print('------------------------')\n",
    "    # \n",
    "#    print('scored ', len(scored_mats))\n",
    "    pred_indices = [i for i, w in enumerate(instance.words) if w.is_prd]\n",
    "    num_preds = len(pred_indices)\n",
    "    pases = []\n",
    "    for pred_id, scored_mat in zip(pred_indices, scored_mats):\n",
    "        #        print(pred_id)\n",
    "        pred = Predicate(word_index=pred_id,\n",
    "                         word_form=instance.words[pred_id].form)\n",
    "        args = []\n",
    "        for slot, slot_id in zip(arg_types, range(num_types)):\n",
    "            if slot != 'NIL':\n",
    "                max_index = np.argmax(scored_mat[slot_id])\n",
    "#                print(slot, ': ', max_index)\n",
    "                args.append(Argument(word_index=max_index,\n",
    "                                     word_form=instance.words[max_index].form,\n",
    "                                     arg_type=slot))\n",
    "                #                print('\\t', max_index)\n",
    "        pases.append(PAS(pred, args))\n",
    "#        print('psize: ', len(pases))\n",
    "    return Sentence(instance.words, pases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "   2: 5.136\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "   4: 2.509\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "   6: 1.233\n",
      "Prec = (1/27) = 3.7037037037037037\n",
      "Rec  = (1/15) = 6.666666666666667\n",
      "   8: 1.038\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  10: 0.697\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  12: 0.628\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  14: 0.585\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  16: 0.559\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  18: 0.545\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  20: 0.546\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  22: 0.537\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  24: 0.528\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  26: 0.525\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  28: 0.524\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  30: 0.524\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  32: 0.524\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  34: 0.523\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  36: 0.523\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  38: 0.523\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  40: 0.523\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  42: 0.523\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  44: 0.523\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  46: 0.523\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  48: 0.522\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  50: 0.522\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  52: 0.522\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  54: 0.522\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  56: 0.522\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  58: 0.522\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  60: 0.522\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  62: 0.522\n",
      "Prec = (0/27) = 0.0\n",
      "Rec  = (0/15) = 0.0\n",
      "  64: 0.522\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-0c2f9d2f320d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0msent_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msent_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mpred_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpred_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mgold_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgold_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         })\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#        print(mat.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from bp_pas.eval import evaluate\n",
    "from bp_pas.ling.pas import PAS, Predicate, Argument\n",
    "from bp_pas.ling.sent import Sentence\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "loss = model.loss_even_newer(gold_placeholder, scored_mat_op.output)\n",
    "#loss = model.loss_old(gold_placeholder, pred_placeholder, pred_embeddings, rep)\n",
    "opt_op = opt.minimize(loss)\n",
    "\n",
    "num_epochs = 3000\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for train_dict in train_dicts:\n",
    "        sent_ids, pred_ids, gold_labels = train_dict\n",
    "#        print(gold_labels.shape)\n",
    "        _, closs = sess.run([opt_op, loss], feed_dict = {\n",
    "#        mat = sess.run(scored_mat_op, feed_dict = {\n",
    "            sent_placeholder: sent_ids,\n",
    "            pred_placeholder: pred_ids,\n",
    "            gold_placeholder: gold_labels\n",
    "        })\n",
    "#        print(mat.shape)\n",
    "        epoch_loss += closs.mean()\n",
    "    if epoch > 0 and epoch % 2 == 0:\n",
    "        decoded = [pas\n",
    "                   for tsent in train_data\n",
    "                   for pas in meta_decode(sess, tsent).pas ]#[0:1]\n",
    "#        decoded = [pas \n",
    "#                   for tsent, tdict in zip(train_data, train_dicts)\n",
    "#                   for pas in meta_decode(sess, tsent)] # #model.decode(sess, tsent, tdict).pas]\n",
    "        gold = [pas \n",
    "                for sent in train_data\n",
    "                for pas in sent.pas]#[0:1]\n",
    "        evaluate(decoded, gold, verbose=False)\n",
    "        print('{0:>4}: {1:.3f}'.format(epoch, epoch_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "# setup fonts\n",
    "from __future__ import unicode_literals\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Tahoma']\n",
    "from matplotlib import rc\n",
    "#matplotlib.rcParams['text.usetex'] = True\n",
    "rc('text', usetex=False)\n",
    "matplotlib.rcParams['text.latex.unicode'] = True\n",
    "#rc('text', usetex=True)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#from matplotlib import rcParams\n",
    "\n",
    "#import matplotlib.font_manager as fm\n",
    "#from matplotlib.font_manager import FontProperties\n",
    "#fp = FontProperties(fname='/Users/narad/Library/Fonts/ipag.ttf', size=14)\n",
    "#rcParams['font.family'] = fp.get_name()\n",
    "#prop = fm.FontProperties(fname='/Users/user/Downloads/IPAfont00303/ipag.ttf')\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.rcParams['text.usetex'] = True\n",
    "#matplotlib.rcParams['text.latex.unicode'] = True\n",
    "#matplotlib.rc('font', **{'sans-serif' : 'AppleGothic',\n",
    "#                           'family' : 'sans-serif'})\n",
    "\n",
    "\n",
    "def heatmap_plot_comp(sess, instance):\n",
    "    words = [w.form for w in instance.words]\n",
    "    pred_dicts = separate_pred_dicts(instance, pred_vocab, arg_vocab, arg_types, mask_token)\n",
    "    for pred_dict in pred_dicts:\n",
    "        sent_ids, pred_ids, gold_labels = pred_dict\n",
    "        print([arg_vocab.word(sid) for sid in sent_ids])\n",
    "        print([pred_vocab.word(pid) for pid in pred_ids])\n",
    "        model_scores, synt_scores, sem_scores, frame = sess.run([scored_mat_op.sigmoid_output, scored_mat_op.synt_scores, scored_mat_op.sem_scores, scored_mat_op.frame], feed_dict = {\n",
    "                sent_placeholder: sent_ids,\n",
    "                pred_placeholder: pred_ids,\n",
    "                gold_placeholder: gold_labels        \n",
    "        })\n",
    "        synt_rep, sem_rep = sess.run([scored_mat_op.synt_rep, scored_mat_op.sem_rep], feed_dict = {\n",
    "                sent_placeholder: sent_ids,\n",
    "                pred_placeholder: pred_ids,\n",
    "                gold_placeholder: gold_labels        \n",
    "        })\n",
    "        debug_mat = sess.run(scored_mat_op.debug_embed_rep, feed_dict = {\n",
    "                sent_placeholder: sent_ids,\n",
    "                pred_placeholder: pred_ids,\n",
    "                gold_placeholder: gold_labels        \n",
    "        })\n",
    "        \n",
    "#        pred_indices = [i for i, w in enumerate(instance.words) if w.is_prd]\n",
    "#        num_preds = len(pred_indices)\n",
    "#        scored_by_pred = np.split(scores, indices_or_sections=num_preds, axis=1)\n",
    "#        gold_by_pred = np.split(gold_labels, indices_or_sections=num_preds, axis=1)\n",
    "        print('Gold scores:')\n",
    "        sentence_heatmap(gold_labels, arg_types, words)\n",
    "        print('Model scores:')\n",
    "        sentence_heatmap(model_scores, arg_types, words)\n",
    "        print('Syntactic/context scores:')\n",
    "        print(synt_scores.shape)\n",
    "        sentence_heatmap(synt_scores, arg_types, words)\n",
    "        print('Semantic scores:')\n",
    "        print(sem_scores.shape)\n",
    "        sentence_heatmap(sem_scores, arg_types, words)\n",
    "        print(frame.shape)\n",
    "        print(frame)\n",
    "        print(synt_rep.shape)\n",
    "        print(synt_rep)\n",
    "        sentence_heatmap(synt_rep.transpose(), list(range(synt_rep.shape[1])), words)\n",
    "        print(sem_rep.shape)\n",
    "        print(debug_mat.shape)\n",
    "        print('\\n-------------------------------------------\\n')\n",
    "\n",
    "\n",
    "\n",
    "def sentence_heatmap(scores, slots, words):\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.imshow(scores, cmap='hot', interpolation='nearest')\n",
    "    \n",
    "    ax1.tick_params(direction='out')\n",
    "    ax1.set_yticks(range(len(slots)))\n",
    "    ax1.set_yticklabels(slots, minor=False)\n",
    "\n",
    "    ax1.set_xticks(range(len(words)))\n",
    "    ax1.set_xticklabels(words, rotation=45, ha='right')\n",
    "\n",
    "#    ax2 = fig.add_subplot(211)\n",
    "#    ax2.imshow(gold_mat, cmap='hot', interpolation='nearest')\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig('/Users/narad/Desktop/test2png.png', dpi=100)\n",
    "#    print(gold_mat)\n",
    "#    print(scored_mat)\n",
    "\n",
    "heatmap_plot_comp(sess, train_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tsent in test_data:\n",
    "    decoded = [pas\n",
    "               for tsent in train_data\n",
    "               for pas in meta_decode(sess, tsent).pas ]\n",
    "    gold = [pas \n",
    "            for sent in train_data\n",
    "            for pas in sent.pas]\n",
    "    evaluate(decoded, gold, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#feed_dict = {sent_placeholder: sent_ids, pred_placeholder: pred_ids}\n",
    "#context_out = sess.run(context_rep, feed_dict=feed_dict)\n",
    "#print(context_out.shape)\n",
    "#out = sess.run(pred_scores, feed_dict=feed_dict)\n",
    "#print(out)\n",
    "#print(out.shape)\n",
    "#print(out[0].shape)\n",
    "\n",
    "# 1, 9, 8\n",
    "\n",
    "#want 1, 9, 1\n",
    "\n",
    "# context mat is 71, 8\n",
    "\n",
    "# pred mat is 8, 36\n",
    "\n",
    "# after mult is 71 x 36\n",
    "\n",
    "#def pred_single_scoring(single_pred_set, single_context, num_slots, num_preds):\n",
    "#    return single_pred_set\n",
    "#    single_pred = tf.unstack(single_pred_set, num=num_preds)[0]\n",
    "#    return tf.transpose(single_pred)\n",
    "#    return tf.matmul(single_context, tf.transpose(single_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#def pas2matrix(pas):\n",
    "    \n",
    "\n",
    "\n",
    "#def sents2batch(sents, vocab, batch_size):\n",
    "#    for sent in sents:\n",
    "#\n",
    "#    int_sents = [ for sent in train_data[0]]\n",
    "#    return np.array([int_sents[0]]), np.array([int_sents[0]])\n",
    "\n",
    "\n",
    "#sents, labels = sents2batch(train_data, vocab, batch_size=1)\n",
    "#print(batched_sent_dicts.shape)\n",
    "\n",
    "d = train_data[0][0]\n",
    "#print(len(d))\n",
    "#print(' '.join([w.form for w in d.words]))\n",
    "#for w in d:\n",
    "#    if w.is_prd:\n",
    "#        print(w, '\\t', w.arg_types, '\\t  ', w.arg_indices)\n",
    "#sent2nump(d)\n",
    "\n",
    "#GA_INDEX = 0\n",
    "#O_INDEX = 1\n",
    "#NI_INDEX = 2\n",
    "\n",
    "# pred id 10 has NI and GA\n",
    "# pred id 25 has GA and O\n",
    "# pred id 31 has GA\n",
    "\n",
    "#sent_ids, pred_ids = sent2nump(train_data[0][0])\n",
    "#print(sent_ids)\n",
    "#print(pred_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def context_embeddings(sent, arg_embeddings, output_dim):\n",
    "    # Shape [batch_size, max_sent_len, emb_dim]\n",
    "    embed_rep = tf.gather(arg_embeddings, sent)\n",
    "    # List of length max_sent_len, comprising [batch_size, emb_dim] tensors\n",
    "    X = embed_rep\n",
    "    fw_cell = tf.contrib.rnn.LSTMCell(num_units=output_dim/2, state_is_tuple=True)\n",
    "    bw_cell = tf.contrib.rnn.LSTMCell(num_units=output_dim/2, state_is_tuple=True)\n",
    "    outputs, states  = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=fw_cell,\n",
    "        cell_bw=bw_cell,\n",
    "        dtype=tf.float64,\n",
    "#        sequence_length=X_lengths,\n",
    "        inputs=X)\n",
    "    output_fw, output_bw = outputs\n",
    "    states_fw, states_bw = states\n",
    "    final_rep = tf.concat(outputs, 2)\n",
    "    return final_rep\n",
    "\n",
    "def complete_embeddings(sent, sem_arg_embeddings, synt_arg_embeddings, synt_output_dim):\n",
    "    # Embed and unbatch\n",
    "    sem_embeds = tf.gather(sem_arg_embeddings, sent)\n",
    "    sem_embeds = tf.reshape(sem_embeds, shape=[-1, tf.shape(sem_embeds)[-1]])\n",
    "    # Embed and unbatch\n",
    "    synt_embeds = context_embeddings(sent, synt_arg_embeddings, synt_output_dim)\n",
    "    synt_embeds = tf.reshape(synt_embeds, shape=[-1, tf.shape(synt_embeds)[-1]])\n",
    "    return tf.concat([sem_embeds, synt_embeds], axis=1)\n",
    "\n",
    "def pred_full_scoring(preds, pred_embeddings, rep_mat):\n",
    "    # Reshape the context mat to remove the batch dim\n",
    "    context_tensor = rep_mat #tf.reshape(rep_mat, shape=[-1, tf.shape(rep_mat)[-1]])\n",
    "\n",
    "    # Shape [batch_size, num_tokens, pred_embed_dim]\n",
    "    # actually ,remove batch for now\n",
    "    pred_mat = tf.gather(pred_embeddings, preds)\n",
    "    pred_mat = tf.squeeze(pred_mat)\n",
    "\n",
    "    # Split the pred embeddings into individual pred tensors\n",
    "    score_tensor = tf.map_fn(lambda x: scoring(x, context_tensor), pred_mat)\n",
    "    \n",
    "    # Reshape score tensor for loss tensor\n",
    "    score_tensor = tf.transpose(tf.reshape(score_tensor, shape=[-1, tf.shape(score_tensor)[-1]]))\n",
    "    return score_tensor\n",
    "\n",
    "\n",
    "def scoring(pred_tensor, rep_tensor):\n",
    "    pred_tensor = tf.reshape(pred_tensor, shape=[tf.shape(rep_tensor)[-1], -1])\n",
    "    return tf.matmul(rep_tensor, pred_tensor)\n",
    "\n",
    "\n",
    "def srl_loss(gold, preds, pred_embeddings, rep_mat):\n",
    "    prediction = pred_full_scoring(pred_placeholder, pred_embeddings, rep_mat)\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.transpose(prediction), \n",
    "                                                   labels=tf.transpose(gold))\n",
    "    return loss\n",
    "\n",
    "#rep = context_embeddings(sent_placeholder, arg_embeddings, output_dim=8)\n",
    "rep = complete_embeddings(sent_placeholder,\n",
    "                          sem_arg_embeddings,\n",
    "                          synt_arg_embeddings,\n",
    "                          synt_output_dim=synt_arg_embedding_size)\n",
    "pred_scores = pred_full_scoring(pred_placeholder, pred_embeddings, rep)\n",
    "decode_pred_scores = tf.nn.sigmoid(pred_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setup placeholders\n",
    "#batch_size = 1\n",
    "#sent_batch_placeholder = tf.placeholder(tf.int32, shape=(batch_size, None))\n",
    "#pred_batch_placeholder = tf.placeholder(tf.int32, shape=(batch_size, None))\n",
    "#gold_batch_placeholder = tf.placeholder(tf.float64, shape=(num_types, None))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# rep = context_embeddings(sent_placeholder, arg_embeddings, output_dim=8)\n",
    "# rep = model.complete_embeddings(sent_placeholder,\n",
    "#                           sem_arg_embeddings,\n",
    "#                           synt_arg_embeddings,\n",
    "#                           synt_output_dim=synt_arg_embedding_size)\n",
    "# pred_scores = model.pred_full_scoring(pred_placeholder, pred_embeddings, rep)\n",
    "# decode_pred_scores = tf.nn.sigmoid(pred_scores)\n",
    "\n",
    "# sess = tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# sent_ids, pred_ids, gold_labels = train_dicts[0]\n",
    "# rep_out = sess.run(rep, feed_dict = {\n",
    "#     sent_placeholder: sent_ids,\n",
    "#     pred_placeholder: pred_ids,\n",
    "#     gold_placeholder: gold_labels\n",
    "# })\n",
    "# #print(rep_out)\n",
    "# print(rep_out.shape)\n",
    "\n",
    "# pred_out = sess.run(pred_scores, feed_dict = {\n",
    "#     sent_placeholder: sent_ids,\n",
    "#     pred_placeholder: pred_ids,\n",
    "#     gold_placeholder: gold_labels\n",
    "# })\n",
    "# #print(pred_out)\n",
    "# print(pred_out.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print('scored ', len(scored_mats))\n",
    "#     pred_indices = [i for i, w in enumerate(instance.words) if w.is_prd]\n",
    "#     num_preds = len(pred_indices)\n",
    "# #    scored_by_pred = np.split(scores, indices_or_sections=num_preds, axis=1)\n",
    "#     pases = []\n",
    "#     for pred_id, scored_mat in zip(pred_indices, scored_mats):\n",
    "#         #        print(pred_id)\n",
    "#         pred = Predicate(word_index=pred_id,\n",
    "#                          word_form=instance.words[pred_id].form)\n",
    "#         args = []\n",
    "#         for slot, slot_id in zip(arg_types, range(num_types)):\n",
    "#             if slot != 'NIL':\n",
    "#                 max_index = np.argmax(scored_mat[slot_id])\n",
    "#                 args.append(Argument(word_index=max_index,\n",
    "#                                      word_form=instance.words[max_index].form,\n",
    "#                                      arg_type=slot))\n",
    "#                 #                print('\\t', max_index)\n",
    "#         pases.append(PAS(pred, args))\n",
    "#     return Sentence(instance.words, pases)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
