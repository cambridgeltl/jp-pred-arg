{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bp_pas.params as params\n",
    "import bp_pas.corpus as corpus\n",
    "import bp_pas.stats as stats\n",
    "from bp_pas.eval import evaluate\n",
    "\n",
    "import sys\n",
    "import gflags\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.load_defaults()\n",
    "flags = gflags.FLAGS\n",
    "#arg_str = '' #'--train_data single.utf8 --dev_data single.utf8 --test_data single.utf8'\n",
    "arg_str = '--max_train_instances 1' #'--train_data single.utf8 --dev_data single.utf8 --test_data single.utf8'\n",
    "args = [''] + arg_str.split(' ')\n",
    "argv = flags(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tmp params to add to flags\n",
    "unk_threshold = 10\n",
    "unk_token = '<UNK>'\n",
    "arg_embedding_size = 32\n",
    "pred_embedding_size = 8\n",
    "num_training_instances = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(flags.train_data)\n",
    "ntc = corpus.NTCLoader()\n",
    "train_data = ntc.load_corpus(flags.train_data, flags.max_train_instances)\n",
    "test_data  = ntc.load_corpus(flags.test_data, flags.max_train_instances)\n",
    "dev_data   = ntc.load_corpus(flags.dev_data, flags.max_train_instances)\n",
    "print('{} train sentences.'.format(len(train_data)))\n",
    "print('{} test sentences.'.format(len(test_data)))\n",
    "print('{} dev sentences.'.format(len(dev_data)))\n",
    "stats.corpus_statistics(train_data)\n",
    "stats.show_case_dist(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class Vocab:\n",
    "    \n",
    "    def __init__(self, init_words=[], unk_token = '<UNK>', unk_threshold=10):\n",
    "        self.word2int = {}\n",
    "        self.int2word = []\n",
    "        self.frozen = False\n",
    "        self.add(unk_token)\n",
    "        if len(init_words) > 0:\n",
    "            self.build(init_words, unk_threshold)\n",
    "\n",
    "    def add(self, elem):\n",
    "        assert not self.frozen\n",
    "        if elem not in self.word2int:\n",
    "            self.int2word.append(elem)\n",
    "            self.word2int[elem] = len(self) - 1\n",
    "    \n",
    "    def build(self, words, unk_threshold=10):\n",
    "        counter = Counter()\n",
    "        for w in words:\n",
    "            counter[w] += 1\n",
    "        for w,c in counter.items():\n",
    "            if c > unk_threshold:\n",
    "                self.add(w)\n",
    "    \n",
    "    def freeze(self):\n",
    "        self.frozen = True\n",
    "\n",
    "    def index(self, elem):\n",
    "        if self.frozen and elem not in self.word2int:\n",
    "            return 0\n",
    "        else:\n",
    "            assert elem in self.word2int\n",
    "            return self.word2int[elem]\n",
    "    \n",
    "    def word(self, index):\n",
    "        assert index < self.int2word\n",
    "        return self.int2word[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.int2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup vocabulary\n",
    "all_sents = train_data[0] + test_data[0] + dev_data[0]\n",
    "\n",
    "\n",
    "print('Collecting all tokens...')\n",
    "word_tokens = list(word.form\n",
    "                   for sent in all_sents\n",
    "                   for word in sent)\n",
    "print('Total tokens: {}\\n'.format(len(word_tokens)))\n",
    "\n",
    "print('Building (argument) vocabulary...')\n",
    "arg_vocab = Vocab(init_words=word_tokens, \n",
    "                  unk_token=unk_token, \n",
    "                  unk_threshold=unk_threshold)  #[unk_token] + get_vocab(word_tokens)\n",
    "arg_vocab.freeze()\n",
    "arg_vocabulary_size = len(arg_vocab)\n",
    "print('Vocabulary size: {}\\n'.format(arg_vocabulary_size))\n",
    "\n",
    "print('Collecting predicates...')\n",
    "pred_vocab = Vocab(init_words=[word.form\n",
    "                               for sent in all_sents\n",
    "                               for word in sent if word.is_prd])\n",
    "pred_vocab.freeze()\n",
    "pred_vocabulary_size = len(pred_vocab)\n",
    "print('Number of predicates: {}\\n'.format(pred_vocabulary_size))\n",
    "\n",
    "print('Collecting argument types...')\n",
    "arg_types = ['NIL'] + list(set([arg.arg_type \n",
    "                      for sent in all_sents \n",
    "                      for pas in sent.pas\n",
    "                      for arg in pas.args]))\n",
    "#for sent in train_data[0]:\n",
    "#    for pas in sent.pas:\n",
    "#        print(pas.pred.word_index)\n",
    "#        for arg in pas.args:\n",
    "#            print(arg)\n",
    "print('Arg types: ', arg_types)\n",
    "num_types = len(arg_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data structures into numpy ndarrays for placeholders\n",
    "def sent2nump(sent):\n",
    "    sent_ids = [arg_vocab.index(w.form) for w in sent]\n",
    "    pases = [w.form for w in sent if w.is_prd]\n",
    "    pred_ids = [pred_vocab.index(w.form) for w in sent if w.is_prd]\n",
    "#    print(pas_ids)\n",
    "#    print(pases)\n",
    "    label_mats = []\n",
    "    for pas in sent.pas:\n",
    "        lm = label_matrix(pas, arg_types, len(sent))\n",
    "        label_mats.append(lm)\n",
    "    lm = np.concatenate(label_mats, axis=1) #.shape\n",
    "    return [sent_ids], [pred_ids], lm\n",
    "#        break\n",
    "\n",
    "def label_matrix(pas, arg_types, sent_len):\n",
    "    zeros = np.zeros((len(arg_types), sent_len))\n",
    "    for arg in pas.args:\n",
    "        zeros[arg_types.index(arg.arg_type), arg.word_index] = 1.0\n",
    "    for j in range(sent_len):\n",
    "        found = False\n",
    "        for i in range(1, len(arg_types)):\n",
    "            if zeros[i, j] == 1.0:\n",
    "                found = True\n",
    "        if not found:\n",
    "            zeros[0,j] = 1.0\n",
    "    return zeros\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#def pas2matrix(pas):\n",
    "    \n",
    "\n",
    "def sent2ints(sent):\n",
    "    sent_ids = [vocab.index(w.form) for w in sent]\n",
    "    pas_ids = []\n",
    "    return sent_ids, pas_ids\n",
    "\n",
    "#def sents2batch(sents, vocab, batch_size):\n",
    "#    for sent in sents:\n",
    "#\n",
    "#    int_sents = [ for sent in train_data[0]]\n",
    "#    return np.array([int_sents[0]]), np.array([int_sents[0]])\n",
    "\n",
    "\n",
    "#sents, labels = sents2batch(train_data, vocab, batch_size=1)\n",
    "#print(batched_sent_dicts.shape)\n",
    "\n",
    "d = train_data[0][0]\n",
    "#print(len(d))\n",
    "#print(' '.join([w.form for w in d.words]))\n",
    "#for w in d:\n",
    "#    if w.is_prd:\n",
    "#        print(w, '\\t', w.arg_types, '\\t  ', w.arg_indices)\n",
    "#sent2nump(d)\n",
    "\n",
    "#GA_INDEX = 0\n",
    "#O_INDEX = 1\n",
    "#NI_INDEX = 2\n",
    "\n",
    "# pred id 10 has NI and GA\n",
    "# pred id 25 has GA and O\n",
    "# pred id 31 has GA\n",
    "\n",
    "train_dicts = [sent2nump(td) for td in train_data[0][1:5]]\n",
    "#sent_ids, pred_ids = sent2nump(train_data[0][0])\n",
    "#print(sent_ids)\n",
    "#print(pred_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integerize the sentences\n",
    "print(len(int_sents))\n",
    "print(int_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the context embedding\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Construct embedding matrix for contex/arguments\n",
    "arg_embeddings = tf.Variable(tf.random_uniform([arg_vocabulary_size, arg_embedding_size], -1.0, 1.0, dtype=tf.float64))\n",
    "\n",
    "# Construct embedding matrix for predicates\n",
    "pred_embeddings = tf.Variable(tf.random_uniform([pred_vocabulary_size, num_types * pred_embedding_size], -1.0, 1.0, dtype=tf.float64))\n",
    "\n",
    "# Setup placeholders\n",
    "batch_size = 1\n",
    "sent_placeholder = tf.placeholder(tf.int32, shape=(batch_size, None))\n",
    "pred_placeholder = tf.placeholder(tf.int32, shape=(batch_size, None))\n",
    "gold_placeholder = tf.placeholder(tf.float64, shape=(num_types, None))\n",
    "\n",
    "def context_embeddings(sent, arg_embeddings, output_dim):\n",
    "    # Shape [batch_size, max_sent_len, emb_dim]\n",
    "    embed_rep = tf.gather(arg_embeddings, sent)\n",
    "    # List of length max_sent_len, comprising [batch_size, emb_dim] tensors\n",
    "#    X = tf.unstack(embed_rep, axis=1, num=72)\n",
    "#    return X\n",
    "    X = embed_rep\n",
    "#    return X\n",
    "    fw_cell = tf.contrib.rnn.LSTMCell(num_units=output_dim/2, state_is_tuple=True)\n",
    "    bw_cell = tf.contrib.rnn.LSTMCell(num_units=output_dim/2, state_is_tuple=True)\n",
    "    outputs, states  = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=fw_cell,\n",
    "        cell_bw=bw_cell,\n",
    "        dtype=tf.float64,\n",
    "#        sequence_length=X_lengths,\n",
    "        inputs=X)\n",
    "    output_fw, output_bw = outputs\n",
    "    states_fw, states_bw = states\n",
    "    final_rep = tf.concat(outputs, 2)\n",
    "    return final_rep\n",
    "\n",
    "\n",
    "def pred_full_scoring(preds, pred_embeddings, context_mat):\n",
    "    # Reshape the context mat to remove the batch dim\n",
    "    context_tensor = tf.reshape(context_mat, shape=[-1, tf.shape(context_mat)[-1]])\n",
    "\n",
    "    # Shape [batch_size, num_tokens, pred_embed_dim]\n",
    "    # actually ,remove batch for now\n",
    "    pred_mat = tf.gather(pred_embeddings, preds)\n",
    "    pred_mat = tf.squeeze(pred_mat)\n",
    "\n",
    "    # Split the pred embeddings into individual pred tensors\n",
    "    score_tensor = tf.map_fn(lambda x: scoring(x, context_tensor), pred_mat)\n",
    "    \n",
    "    # Reshape score tensor for loss tensor\n",
    "    score_tensor = tf.transpose(tf.reshape(score_tensor, shape=[-1, tf.shape(score_tensor)[-1]]))\n",
    "    return score_tensor\n",
    "\n",
    "\n",
    "def scoring(pred_tensor, context_tensor):\n",
    "#    pred_tensor = tf.reshape(pred_tensor, shape=[1, -1, 4])\n",
    "    pred_tensor = tf.reshape(pred_tensor, shape=[tf.shape(context_tensor)[-1], -1])\n",
    "    return tf.matmul(context_tensor, pred_tensor)\n",
    "\n",
    "#def pred_single_scoring(single_pred_set, single_context, num_slots, num_preds):\n",
    "#    return single_pred_set\n",
    "#    single_pred = tf.unstack(single_pred_set, num=num_preds)[0]\n",
    "#    return tf.transpose(single_pred)\n",
    "#    return tf.matmul(single_context, tf.transpose(single_pred))\n",
    "\n",
    "def srl_loss(gold, preds, pred_embeddings, context_mat):\n",
    "    prediction = pred_full_scoring(pred_placeholder, pred_embeddings, context_rep)\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.transpose(prediction), \n",
    "                                                   labels=tf.transpose(gold))\n",
    "    return loss\n",
    "\n",
    "context_rep = context_embeddings(sent_placeholder, arg_embeddings, output_dim=8)\n",
    "pred_scores = pred_full_scoring(pred_placeholder, pred_embeddings, context_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding method\n",
    "from bp_pas.ling.pas import PAS, Predicate, Argument\n",
    "from bp_pas.ling.sent import Sentence\n",
    "\n",
    "def decode(sess, instance):\n",
    "    sent_ids, pred_ids, gold_labels = sent2nump(instance)\n",
    "    scores = sess.run(pred_scores, feed_dict = {\n",
    "            sent_placeholder: sent_ids,\n",
    "            pred_placeholder: pred_ids,\n",
    "            gold_placeholder: gold_labels        \n",
    "    })\n",
    "    pred_indices = [i for i, w in enumerate(instance.words) if w.is_prd]\n",
    "    num_preds = len(pred_indices)\n",
    "    scored_by_pred = np.split(scores, indices_or_sections=num_preds, axis=1)\n",
    "    pases = []\n",
    "    for pred_id, scored_mat in zip(pred_indices, scored_by_pred):\n",
    "#        print(pred_id)\n",
    "        pred = Predicate(word_index = pred_id,\n",
    "                         word_form = instance.words[pred_id].form)\n",
    "        args = []\n",
    "        for slot, slot_id in zip(arg_types, range(num_types)):\n",
    "            if slot != 'NIL':\n",
    "                max_index = np.argmax(scored_mat[slot_id])\n",
    "                args.append(Argument(word_index=max_index,\n",
    "                                     word_form=instance.words[max_index].form,\n",
    "                                     arg_type=slot))\n",
    "#                print('\\t', max_index)\n",
    "        pases.append(PAS(pred, args))\n",
    "    return Sentence(instance.words, pases)\n",
    "\n",
    "decode(sess, train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bp_pas.eval import evaluate\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "loss = srl_loss(gold_placeholder, pred_placeholder, pred_embeddings, context_rep)\n",
    "opt_op = opt.minimize(loss)\n",
    "\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for train_dict in train_dicts:\n",
    "        sent_ids, pred_ids, gold_labels = train_dict\n",
    "#        print(gold_labels.shape)\n",
    "        _, closs = sess.run([opt_op, loss], feed_dict = {\n",
    "            sent_placeholder: sent_ids,\n",
    "            pred_placeholder: pred_ids,\n",
    "            gold_placeholder: gold_labels\n",
    "        })\n",
    "        epoch_loss += closs.mean()\n",
    "    if epoch % 100 == 0:\n",
    "        decoded = [pas for pas in decode(sess, sent).pas for sent in train_data[0][:5]]\n",
    "        gold = [pas for pas in sent.pas for sent in train_data[0][:5]]\n",
    "        evaluate(decoded, gold)\n",
    "        print('{0:>4}: {1:.2f}'.format(epoch, epoch_loss))\n",
    "    \n",
    "\n",
    "#feed_dict = {sent_placeholder: sent_ids, pred_placeholder: pred_ids}\n",
    "#context_out = sess.run(context_rep, feed_dict=feed_dict)\n",
    "#print(context_out.shape)\n",
    "#out = sess.run(pred_scores, feed_dict=feed_dict)\n",
    "#print(out)\n",
    "#print(out.shape)\n",
    "#print(out[0].shape)\n",
    "\n",
    "# 1, 9, 8\n",
    "\n",
    "#want 1, 9, 1\n",
    "\n",
    "\n",
    "\n",
    "# context mat is 71, 8\n",
    "\n",
    "# pred mat is 8, 36\n",
    "\n",
    "# after mult is 71 x 36\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding method\n",
    "from bp_pas.ling.pas import PAS, Predicate, Argument\n",
    "from bp_pas.ling.sent import Sentence\n",
    "\n",
    "def decode(sess, instance):\n",
    "    sent_ids, pred_ids, gold_labels = sent2nump(instance)\n",
    "    scores = sess.run(pred_scores, feed_dict = {\n",
    "            sent_placeholder: sent_ids,\n",
    "            pred_placeholder: pred_ids,\n",
    "            gold_placeholder: gold_labels        \n",
    "    })\n",
    "    pred_indices = [i for i, w in enumerate(instance.words) if w.is_prd]\n",
    "    num_preds = len(pred_indices)\n",
    "    scored_by_pred = np.split(scores, indices_or_sections=num_preds, axis=1)\n",
    "    pases = []\n",
    "    for pred_id, scored_mat in zip(pred_indices, scored_by_pred):\n",
    "#        print(pred_id)\n",
    "        pred = Predicate(word_index = pred_id,\n",
    "                         word_form = instance.words[pred_id].form)\n",
    "        args = []\n",
    "        for slot, slot_id in zip(arg_types, range(num_types)):\n",
    "            if slot != 'NIL':\n",
    "                max_index = np.argmax(scored_mat[slot_id])\n",
    "                args.append(Argument(word_index=max_index,\n",
    "                                     word_form=instance.words[max_index].form,\n",
    "                                     arg_type=slot))\n",
    "#                print('\\t', max_index)\n",
    "        pases.append(PAS(pred, args))\n",
    "    return Sentence(instance.words, pases)\n",
    "\n",
    "decode(sess, train_data[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
